# ğŸš€ Desafio MBA Engenharia de Software com IA: IngestÃ£o e Busca SemÃ¢ntica com LangChain e PostgreSQL

Este desafio visa construir um sistema de Q&A (Perguntas e Respostas) baseado em um documento PDF, utilizando o framework **LangChain** e o banco de dados vetorial **pgVector** no PostgreSQL.

## ğŸ¯ Objetivo

Desenvolver um software com duas funcionalidades principais:

1.  **IngestÃ£o:** Ler um arquivo PDF e persistir seus conteÃºdos como *embeddings* (vetores) em um banco de dados PostgreSQL com pgVector.
2.  **Busca (CLI):** Criar uma interface de linha de comando (CLI) que permita ao usuÃ¡rio fazer perguntas e receber respostas fundamentadas **exclusivamente** no conteÃºdo do PDF ingerido.

### Exemplo de InteraÃ§Ã£o na CLI

| Tipo | Pergunta | Resposta |
| :--- | :--- | :--- |
| **No Contexto** | `Qual o faturamento da Empresa SuperTechIABrazil?` | `O faturamento foi de 10 milhÃµes de reais.` |
| **Fora do Contexto** | `Quantos clientes temos em 2024?` | `NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta.` |

---

## âœ… Requisitos Detalhados

### 1. IngestÃ£o do PDF (`src/ingest.py`)

* O PDF (`document.pdf`) deve ser dividido usando as seguintes configuraÃ§Ãµes:
    * **Chunks:** `1000` caracteres.
    * **Overlap:** `150` caracteres.
* Cada *chunk* deve ser vetorizado e armazenado no PostgreSQL com `pgVector` usando o componente `PGVector`. 

### 2. Consulta via CLI (`src/chat.py` & `src/search.py`)

* Deve ser implementado um script Python que simula um *chat* no terminal.
* **Fluxo RAG (Retrieval-Augmented Generation):** A pergunta do usuÃ¡rio deve ser vetorizada e usada para buscar os **10 resultados mais relevantes (k=10)** no banco.
* A LLM deve ser chamada com um prompt que inclui o contexto recuperado e segue as **REGRAS** estritas abaixo.

#### ğŸ“ Prompt a ser Utilizado

Este Ã© o template do prompt que serÃ¡ enviado ao LLM, contendo o contexto recuperado e as regras de resposta:

```markdown
CONTEXTO: {resultados concatenados do banco de dados}

REGRAS:

Responda somente com base no CONTEXTO.

Se a informaÃ§Ã£o nÃ£o estiver explicitamente no CONTEXTO, responda: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

Nunca invente ou use conhecimento externo.

Nunca produza opiniÃµes ou interpretaÃ§Ãµes alÃ©m do que estÃ¡ escrito.

EXEMPLOS DE PERGUNTAS FORA DO CONTEXTO: Pergunta: "Qual Ã© a capital da FranÃ§a?" Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

Pergunta: "VocÃª acha isso bom ou ruim?" Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

PERGUNTA DO USUÃRIO: {pergunta do usuÃ¡rio}

RESPONDA A "PERGUNTA DO USUÃRIO"
```

---

## âš™ï¸ Tecnologias ObrigatÃ³rias

| Categoria | Tecnologia |
| :--- | :--- |
| **Linguagem** | Python |
| **Framework** | LangChain |
| **Banco de Dados** | PostgreSQL + pgVector |
| **ExecuÃ§Ã£o DB** | Docker & Docker Compose |

## ğŸ“¦ Pacotes Recomendados (LangChain)

| MÃ³dulo | Componente |
| :--- | :--- |
| **Split** | `RecursiveCharacterTextSplitter` |
| **Embeddings (Gemini)** | `GoogleGenerativeAIEmbeddings` |
| **PDF Loader** | `PyPDFLoader` |
| **Vector Store** | `PGVector` |
| **Busca** | `similarity_search_with_score(query, k=10)` |

## ğŸ”‘ ConfiguraÃ§Ã£o API Key

### Gemini (Google)

* **Modelo de Embeddings:** `models/embedding-001`
* **Modelo de LLM para Resposta:** `gemini-2.5-flash-lite`

As chaves de API devem ser configuradas em um arquivo `.env` (baseado no `.env.example`).

---

## ğŸ“‚ Estrutura ObrigatÃ³ria do Projeto

```
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt      # DependÃªncias
â”œâ”€â”€ .env.example          # Template da variÃ¡vel GOOGLE_API_KEY
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py         # Script de ingestÃ£o do PDF
â”‚   â”œâ”€â”€ search.py         # Script de busca
â”‚   â””â”€â”€ chat.py           # CLI para interaÃ§Ã£o com usuÃ¡rio
â”œâ”€â”€ document.pdf          # PDF para ingestÃ£o
â””â”€â”€ README.md             # InstruÃ§Ãµes de execuÃ§Ã£o
```

## ğŸš€ Ordem de ExecuÃ§Ã£o

### 1. Configurar Ambiente Virtual e DependÃªncias

Esta etapa garante que o projeto utilize as versÃµes corretas de Python e das bibliotecas necessÃ¡rias.

| Item | AÃ§Ã£o | Comandos (Linux/macOS) | Comandos (Windows Powershell) |
| :--- | :--- | :--- | :--- |
| **CriaÃ§Ã£o do Venv** | Criar um ambiente virtual isolado. | `python3 -m venv venv` | `python -m venv venv` |
| **AtivaÃ§Ã£o** | Ativar o ambiente virtual. | `source venv/bin/activate` | `.\venv\Scripts\activate` |
| **InstalaÃ§Ã£o** | Instalar todas as dependÃªncias do `requirements.txt`. | `pip install -r requirements.txt` | `pip install -r requirements.txt` |


2.  **Subir o Banco de Dados:** `docker compose up -d`

Utilize o Docker Compose fornecido para inicializar o PostgreSQL com `pgVector`:

```bash
docker compose up -d

3.  **Executar IngestÃ£o do PDF:** `python src/ingest.py`
4.  **Rodar o Chat:** `python src/chat.py`